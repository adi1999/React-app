{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5676715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.document_loaders import ConfluenceLoader\n",
    "# confluence_url = \"YOUR CONFLUENCE URL\"\n",
    "# confluence_token = \"YOUR CONFLUENCE TOKEN\"\n",
    "# loader = ConfluenceLoader(url=confluence_url,token=confluence_token)\n",
    "# documents = loader.load(page_ids=[\"12345\"])\n",
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=500,\n",
    "#     chunk_overlap=20,\n",
    "#     length_function=len,\n",
    "#     add_start_index=True)\n",
    "\n",
    "# chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# embeddings = OpenAIEmbeddings(deployment=\"text-embedding-ada-model\", chunk_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9103d552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pickle\n",
    "import time\n",
    "import langchain\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.chains.qa_with_sources.loading import load_qa_with_sources_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import UnstructuredURLLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d5c6833",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredURLLoader(\n",
    "    urls = [\n",
    "        \"https://python.langchain.com/docs/integrations/platforms/huggingface\",\n",
    "    ]\n",
    ")\n",
    "data = loader.load() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ad2dcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "925286fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Providers\\n\\nHugging Face\\n\\nHugging Face\\n\\nAll functionality related to the Hugging Face Platform.\\n\\nLLMs\\u200b\\n\\nHugging Face Hub\\u200b', metadata={'source': 'https://python.langchain.com/docs/integrations/platforms/huggingface'}),\n",
       " Document(page_content='The Hugging Face Hub is a platform\\nwith over 350k models, 75k datasets, and 150k demo apps (Spaces), all open source\\nand publicly available, in an online platform where people can easily', metadata={'source': 'https://python.langchain.com/docs/integrations/platforms/huggingface'}),\n",
       " Document(page_content='collaborate and build ML together. The Hub works as a central place where anyone\\ncan explore, experiment, collaborate, and build technology with Machine Learning.', metadata={'source': 'https://python.langchain.com/docs/integrations/platforms/huggingface'}),\n",
       " Document(page_content='To use, we should have the huggingface_hub python package installed.\\n\\npip\\n\\ninstall\\n\\nhuggingface_hub\\n\\nSee a usage example.\\n\\nfrom\\n\\nlangchain_community\\n\\nllms\\n\\nimport\\n\\nHuggingFaceHub', metadata={'source': 'https://python.langchain.com/docs/integrations/platforms/huggingface'}),\n",
       " Document(page_content='llms\\n\\nimport\\n\\nHuggingFaceHub\\n\\nHugging Face Local Pipelines\\u200b\\n\\nHugging Face models can be run locally through the HuggingFacePipeline class.\\n\\nWe need to install transformers python package.\\n\\npip', metadata={'source': 'https://python.langchain.com/docs/integrations/platforms/huggingface'}),\n",
       " Document(page_content='pip\\n\\ninstall\\n\\ntransformers\\n\\nSee a usage example.\\n\\nfrom\\n\\nlangchain_community\\n\\nllms\\n\\nhuggingface_pipeline\\n\\nimport\\n\\nHuggingFacePipeline\\n\\nHugging Face TextGen Inference\\u200b', metadata={'source': 'https://python.langchain.com/docs/integrations/platforms/huggingface'}),\n",
       " Document(page_content='Hugging Face TextGen Inference\\u200b\\n\\nText Generation Inference is\\na Rust, Python and gRPC server for text generation inference. Used in production at\\nHuggingFace to power LLMs api-inference widgets.', metadata={'source': 'https://python.langchain.com/docs/integrations/platforms/huggingface'}),\n",
       " Document(page_content='We need to install text_generation python package.\\n\\npip\\n\\ninstall\\n\\ntext_generation\\n\\nSee a usage example.\\n\\nfrom\\n\\nlangchain_community\\n\\nllms\\n\\nimport\\n\\nHuggingFaceTextGenInference\\n\\nChat models\\u200b', metadata={'source': 'https://python.langchain.com/docs/integrations/platforms/huggingface'}),\n",
       " Document(page_content='HuggingFaceTextGenInference\\n\\nChat models\\u200b\\n\\nModels from Hugging Face\\u200b\\n\\nWe can use the Hugging Face LLM classes or directly use the ChatHuggingFace class.\\n\\nWe need to install several python packages.', metadata={'source': 'https://python.langchain.com/docs/integrations/platforms/huggingface'}),\n",
       " Document(page_content='We need to install several python packages.\\n\\npip\\n\\ninstall\\n\\nhuggingface_hub\\n\\npip\\n\\ninstall\\n\\ntransformers\\n\\nSee a usage example.\\n\\nfrom\\n\\nlangchain_community\\n\\nchat_models\\n\\nhuggingface\\n\\nimport', metadata={'source': 'https://python.langchain.com/docs/integrations/platforms/huggingface'}),\n",
       " Document(page_content='chat_models\\n\\nhuggingface\\n\\nimport\\n\\nChatHuggingFace\\n\\nEmbedding Models\\u200b\\n\\nHugging Face Hub\\u200b', metadata={'source': 'https://python.langchain.com/docs/integrations/platforms/huggingface'}),\n",
       " Document(page_content='The Hugging Face Hub is a platform\\nwith over 350k models, 75k datasets, and 150k demo apps (Spaces), all open source\\nand publicly available, in an online platform where people can easily', metadata={'source': 'https://python.langchain.com/docs/integrations/platforms/huggingface'}),\n",
       " Document(page_content='collaborate and build ML together. The Hub works as a central place where anyone\\ncan explore, experiment, collaborate, and build technology with Machine Learning.', metadata={'source': 'https://python.langchain.com/docs/integrations/platforms/huggingface'}),\n",
       " Document(page_content='We need to install the sentence_transformers python package.\\n\\npip\\n\\ninstall\\n\\nsentence_transformers\\n\\nHuggingFaceEmbeddings\\u200b\\n\\nSee a usage example.\\n\\nfrom\\n\\nlangchain_community\\n\\nembeddings\\n\\nimport', metadata={'source': 'https://python.langchain.com/docs/integrations/platforms/huggingface'}),\n",
       " Document(page_content='from\\n\\nlangchain_community\\n\\nembeddings\\n\\nimport\\n\\nHuggingFaceEmbeddings\\n\\nHuggingFaceInstructEmbeddings\\u200b\\n\\nSee a usage example.\\n\\nfrom\\n\\nlangchain_community\\n\\nembeddings\\n\\nimport', metadata={'source': 'https://python.langchain.com/docs/integrations/platforms/huggingface'}),\n",
       " Document(page_content='from\\n\\nlangchain_community\\n\\nembeddings\\n\\nimport\\n\\nHuggingFaceInstructEmbeddings\\n\\nHuggingFaceBgeEmbeddings\\u200b', metadata={'source': 'https://python.langchain.com/docs/integrations/platforms/huggingface'}),\n",
       " Document(page_content='BGE models on the HuggingFace are the best open-source embedding models.', metadata={'source': 'https://python.langchain.com/docs/integrations/platforms/huggingface'}),\n",
       " Document(page_content='BGE model is created by the Beijing Academy of Artificial Intelligence (BAAI). BAAI is a private non-profit organization engaged in AI research and development.', metadata={'source': 'https://python.langchain.com/docs/integrations/platforms/huggingface'}),\n",
       " Document(page_content='See a usage example.\\n\\nfrom\\n\\nlangchain_community\\n\\nembeddings\\n\\nimport\\n\\nHuggingFaceBgeEmbeddings\\n\\nHugging Face Text Embeddings Inference (TEI)\\u200b', metadata={'source': 'https://python.langchain.com/docs/integrations/platforms/huggingface'}),\n",
       " Document(page_content='Hugging Face Text Embeddings Inference (TEI)\\u200b\\n\\nHugging Face Text Embeddings Inference (TEI) is a toolkit for deploying and serving open-source\\ntext embeddings and sequence classification models.', metadata={'source': 'https://python.langchain.com/docs/integrations/platforms/huggingface'}),\n",
       " Document(page_content='We need to install huggingface-hub python package.\\n\\npip\\n\\ninstall\\n\\nhuggingface-hub\\n\\nSee a usage example.\\n\\nfrom\\n\\nlangchain_community\\n\\nembeddings\\n\\nimport\\n\\nHuggingFaceHubEmbeddings\\n\\nDocument Loaders\\u200b', metadata={'source': 'https://python.langchain.com/docs/integrations/platforms/huggingface'}),\n",
       " Document(page_content='HuggingFaceHubEmbeddings\\n\\nDocument Loaders\\u200b\\n\\nHugging Face dataset\\u200b', metadata={'source': 'https://python.langchain.com/docs/integrations/platforms/huggingface'}),\n",
       " Document(page_content='Hugging Face Hub is home to over 75,000\\ndatasets in more than 100 languages\\nthat can be used for a broad range of tasks across NLP, Computer Vision, and Audio.', metadata={'source': 'https://python.langchain.com/docs/integrations/platforms/huggingface'}),\n",
       " Document(page_content='They used for a diverse range of tasks such as translation, automatic speech\\nrecognition, and image classification.', metadata={'source': 'https://python.langchain.com/docs/integrations/platforms/huggingface'}),\n",
       " Document(page_content='We need to install datasets python package.\\n\\npip\\n\\ninstall\\n\\ndatasets\\n\\nSee a usage example.\\n\\nfrom\\n\\nlangchain_community\\n\\ndocument_loaders\\n\\nhugging_face_dataset\\n\\nimport\\n\\nHuggingFaceDatasetLoader\\n\\nTools\\u200b', metadata={'source': 'https://python.langchain.com/docs/integrations/platforms/huggingface'}),\n",
       " Document(page_content='import\\n\\nHuggingFaceDatasetLoader\\n\\nTools\\u200b\\n\\nHugging Face Hub Tools\\u200b\\n\\nHugging Face Tools\\nsupport text I/O and are loaded using the load_huggingface_tool function.', metadata={'source': 'https://python.langchain.com/docs/integrations/platforms/huggingface'}),\n",
       " Document(page_content='We need to install several python packages.\\n\\npip\\n\\ninstall\\n\\ntransformers huggingface_hub\\n\\nSee a usage example.\\n\\nfrom\\n\\nlangchain\\n\\nagents\\n\\nimport\\n\\nload_huggingface_tool\\n\\nPreviousGoogle\\n\\nNextMicrosoft', metadata={'source': 'https://python.langchain.com/docs/integrations/platforms/huggingface'}),\n",
       " Document(page_content='PreviousGoogle\\n\\nNextMicrosoft\\n\\nLLMsHugging Face HubHugging Face Local PipelinesHugging Face TextGen Inference\\n\\nChat modelsModels from Hugging Face', metadata={'source': 'https://python.langchain.com/docs/integrations/platforms/huggingface'}),\n",
       " Document(page_content='Chat modelsModels from Hugging Face\\n\\nEmbedding ModelsHugging Face HubHugging Face Text Embeddings Inference (TEI)\\n\\nDocument LoadersHugging Face dataset\\n\\nToolsHugging Face Hub Tools', metadata={'source': 'https://python.langchain.com/docs/integrations/platforms/huggingface'})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b96c476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da94e3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorindex_hf = FAISS.from_documents(docs, hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6097e1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=\"vector_index.pkl\"\n",
    "with open(file_path, \"wb\") as f:\n",
    "    pickle.dump(vectorindex_hf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf9f0dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=\"vector_index.pkl\"\n",
    "with open(file_path, \"rb\") as f:\n",
    "    vectorIndex = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67ced1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0670a32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCpp(\n",
    "    model_path='C:/Users/allot/Desktop/LLM/chatbot/mistral-7b-openorca.Q4_0.gguf', temperature=0.8,\n",
    "    verbose=True,\n",
    "    n_ctx=4096\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd885248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalQAWithSourcesChain(combine_documents_chain=MapReduceDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template='Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\n{context}\\nQuestion: {question}\\nRelevant text, if any:'), llm=LlamaCpp(client=<llama_cpp.llama.Llama object at 0x0000016B7E7C46A0>, model_path='C:/Users/allot/Desktop/LLM/chatbot/mistral-7b-openorca.Q4_0.gguf', n_ctx=4096)), reduce_documents_chain=ReduceDocumentsChain(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['summaries', 'question'], template='Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \\nIf you don\\'t know the answer, just say that you don\\'t know. Don\\'t try to make up an answer.\\nALWAYS return a \"SOURCES\" part in your answer.\\n\\nQUESTION: Which state/country\\'s law governs the interpretation of the contract?\\n=========\\nContent: This Agreement is governed by English law and the parties submit to the exclusive jurisdiction of the English courts in  relation to any dispute (contractual or non-contractual) concerning this Agreement save that either party may apply to any court for an  injunction or other relief to protect its Intellectual Property Rights.\\nSource: 28-pl\\nContent: No Waiver. Failure or delay in exercising any right or remedy under this Agreement shall not constitute a waiver of such (or any other)  right or remedy.\\n\\n11.7 Severability. The invalidity, illegality or unenforceability of any term (or part of a term) of this Agreement shall not affect the continuation  in force of the remainder of the term (if any) and this Agreement.\\n\\n11.8 No Agency. Except as expressly stated otherwise, nothing in this Agreement shall create an agency, partnership or joint venture of any  kind between the parties.\\n\\n11.9 No Third-Party Beneficiaries.\\nSource: 30-pl\\nContent: (b) if Google believes, in good faith, that the Distributor has violated or caused Google to violate any Anti-Bribery Laws (as  defined in Clause 8.5) or that such a violation is reasonably likely to occur,\\nSource: 4-pl\\n=========\\nFINAL ANSWER: This Agreement is governed by English law.\\nSOURCES: 28-pl\\n\\nQUESTION: What did the president say about Michael Jackson?\\n=========\\nContent: Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\n\\nSix days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\n\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\n\\nHe met the Ukrainian people. \\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. \\n\\nGroups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland.\\nSource: 0-pl\\nContent: And we won’t stop. \\n\\nWe have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life. \\n\\nLet’s use this moment to reset. Let’s stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.  \\n\\nLet’s stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.  \\n\\nWe can’t change how divided we’ve been. But we can change how we move forward—on COVID-19 and other issues we must face together. \\n\\nI recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. \\n\\nThey were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \\n\\nOfficer Mora was 27 years old. \\n\\nOfficer Rivera was 22. \\n\\nBoth Dominican Americans who’d grown up on the same streets they later chose to patrol as police officers. \\n\\nI spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves.\\nSource: 24-pl\\nContent: And a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.  \\n\\nTo all Americans, I will be honest with you, as I’ve always promised. A Russian dictator, invading a foreign country, has costs around the world. \\n\\nAnd I’m taking robust action to make sure the pain of our sanctions  is targeted at Russia’s economy. And I will use every tool at our disposal to protect American businesses and consumers. \\n\\nTonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.  \\n\\nAmerica will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.  \\n\\nThese steps will help blunt gas prices here at home. And I know the news about what’s happening can seem alarming. \\n\\nBut I want you to know that we are going to be okay.\\nSource: 5-pl\\nContent: More support for patients and families. \\n\\nTo get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health. \\n\\nIt’s based on DARPA—the Defense Department project that led to the Internet, GPS, and so much more.  \\n\\nARPA-H will have a singular purpose—to drive breakthroughs in cancer, Alzheimer’s, diabetes, and more. \\n\\nA unity agenda for the nation. \\n\\nWe can do this. \\n\\nMy fellow Americans—tonight , we have gathered in a sacred space—the citadel of our democracy. \\n\\nIn this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done great things. \\n\\nWe have fought for freedom, expanded liberty, defeated totalitarianism and terror. \\n\\nAnd built the strongest, freest, and most prosperous nation the world has ever known. \\n\\nNow is the hour. \\n\\nOur moment of responsibility. \\n\\nOur test of resolve and conscience, of history itself. \\n\\nIt is in this moment that our character is formed. Our purpose is found. Our future is forged. \\n\\nWell I know this nation.\\nSource: 34-pl\\n=========\\nFINAL ANSWER: The president did not mention Michael Jackson.\\nSOURCES:\\n\\nQUESTION: {question}\\n=========\\n{summaries}\\n=========\\nFINAL ANSWER:'), llm=LlamaCpp(client=<llama_cpp.llama.Llama object at 0x0000016B7E7C46A0>, model_path='C:/Users/allot/Desktop/LLM/chatbot/mistral-7b-openorca.Q4_0.gguf', n_ctx=4096)), document_prompt=PromptTemplate(input_variables=['page_content', 'source'], template='Content: {page_content}\\nSource: {source}'), document_variable_name='summaries')), document_variable_name='context'), retriever=VectorStoreRetriever(tags=['FAISS'], vectorstore=<langchain.vectorstores.faiss.FAISS object at 0x0000016B4704F4F0>))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RetrievalQAWithSourcesChain.from_llm(llm=llm, retriever=vectorIndex.as_retriever())\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54a4f301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"what is an embedding?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input_list\": [\n",
      "    {\n",
      "      \"context\": \"from\\n\\nlangchain_community\\n\\nembeddings\\n\\nimport\\n\\nHuggingFaceInstructEmbeddings\\n\\nHuggingFaceBgeEmbeddings​\",\n",
      "      \"question\": \"what is an embedding?\"\n",
      "    },\n",
      "    {\n",
      "      \"context\": \"See a usage example.\\n\\nfrom\\n\\nlangchain_community\\n\\nembeddings\\n\\nimport\\n\\nHuggingFaceBgeEmbeddings\\n\\nHugging Face Text Embeddings Inference (TEI)​\",\n",
      "      \"question\": \"what is an embedding?\"\n",
      "    },\n",
      "    {\n",
      "      \"context\": \"from\\n\\nlangchain_community\\n\\nembeddings\\n\\nimport\\n\\nHuggingFaceEmbeddings\\n\\nHuggingFaceInstructEmbeddings​\\n\\nSee a usage example.\\n\\nfrom\\n\\nlangchain_community\\n\\nembeddings\\n\\nimport\",\n",
      "      \"question\": \"what is an embedding?\"\n",
      "    },\n",
      "    {\n",
      "      \"context\": \"BGE models on the HuggingFace are the best open-source embedding models.\",\n",
      "      \"question\": \"what is an embedding?\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 5:llm:LlamaCpp] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\nfrom\\n\\nlangchain_community\\n\\nembeddings\\n\\nimport\\n\\nHuggingFaceInstructEmbeddings\\n\\nHuggingFaceBgeEmbeddings​\\nQuestion: what is an embedding?\\nRelevant text, if any:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 6:llm:LlamaCpp] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\nSee a usage example.\\n\\nfrom\\n\\nlangchain_community\\n\\nembeddings\\n\\nimport\\n\\nHuggingFaceBgeEmbeddings\\n\\nHugging Face Text Embeddings Inference (TEI)​\\nQuestion: what is an embedding?\\nRelevant text, if any:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 7:llm:LlamaCpp] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\nfrom\\n\\nlangchain_community\\n\\nembeddings\\n\\nimport\\n\\nHuggingFaceEmbeddings\\n\\nHuggingFaceInstructEmbeddings​\\n\\nSee a usage example.\\n\\nfrom\\n\\nlangchain_community\\n\\nembeddings\\n\\nimport\\nQuestion: what is an embedding?\\nRelevant text, if any:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 8:llm:LlamaCpp] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\nBGE models on the HuggingFace are the best open-source embedding models.\\nQuestion: what is an embedding?\\nRelevant text, if any:\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 5:llm:LlamaCpp] [98.38s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" An embedding is a technique that converts data from one representation to another. It's like taking a 3D model and projecting it onto a 2D surface, where each object is given a unique position in the new space according to its features. In the context of natural language processing (NLP), embeddings are particularly useful for capturing the semantic meaning of words or phrases. For instance, an embedding could be used to represent a sentence as a vector, with different components representing aspects like semantics, syntax, and even sentiment. This makes it easier to compare and analyze vast amounts of text data in a way that's suitable for machine learning algorithms.\",\n",
      "        \"generation_info\": null\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 6:llm:LlamaCpp] [98.38s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" An embedding is a representation of data in which the elements within the dataset are transformed into points in a space, allowing for efficient search and analysis. Embeddings can be used for various tasks such as clustering, similarity search, or recommendation systems.\",\n",
      "        \"generation_info\": null\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 7:llm:LlamaCpp] [98.38s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\nAn embedding is a method to represent high-dimensional data in a lower-dimensional space while preserving the structure of the data. This allows for efficient data processing and analysis as it reduces the complexity of the original dataset. Some popular embedding techniques include Word2Vec, GloVe, and fastText for text data, and t-SNE and UMAP for visualizing high-dimensional data in lower dimensions.\\n\\nIn the context of AI and language models, embeddings are often used to map words or sentences into a vector space where semantic meanings can be captured and compared effectively. Embeddings serve as an essential component for various NLP tasks such as question answering, sentiment analysis, text generation, and more.\\n\\nHere's a Python example using HuggingFaceInstructEmbeddings to demonstrate the use of embeddings:\\n\\n```python\\nimport os\\nfrom langchain_community import embeddings\\nfrom HuggingFaceInstructEmbeddings import HuggingFaceInstructEmbeddings\\n\\ndef main():\\n    # Load the embedding model\\n    embedding_model = HuggingFaceInstructEmbeddings.load(\\\"text-embedding-ada-multilingual-10k\\\")\\n\",\n",
      "        \"generation_info\": null\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 8:llm:LlamaCpp] [98.38s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\nAn embedding is a mathematical representation of a dataset where each element in the dataset is transformed into a fixed-size vector.\\n\\nIn the context of natural language processing (NLP), embeddings are used to represent words and sentences as dense vectors that capture their meaning, which can then be used for various downstream tasks like text classification, information retrieval, etc. There are different types of embeddings such as Word Embeddings, Sentence Embeddings, and Document Embeddings.\\n\\nSome popular embedding models include Word2Vec, GloVe, FastText, BERT, ELMO, and XLNet. These models are trained on massive datasets to learn contextualized vector representations for words and sentences. The resulting embeddings can be used in various NLP tasks like sentiment analysis, question answering systems, machine translation, etc.\\n\\nIn summary, an embedding is a way of representing text data in a low-dimensional space, where each word or document is assigned a unique fixed-size vector that reflects its meaning. These vectors are then utilized for numerous downstream tasks within the NLP domain.\",\n",
      "        \"generation_info\": null\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain] [98.38s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"outputs\": [\n",
      "    {\n",
      "      \"text\": \" An embedding is a technique that converts data from one representation to another. It's like taking a 3D model and projecting it onto a 2D surface, where each object is given a unique position in the new space according to its features. In the context of natural language processing (NLP), embeddings are particularly useful for capturing the semantic meaning of words or phrases. For instance, an embedding could be used to represent a sentence as a vector, with different components representing aspects like semantics, syntax, and even sentiment. This makes it easier to compare and analyze vast amounts of text data in a way that's suitable for machine learning algorithms.\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \" An embedding is a representation of data in which the elements within the dataset are transformed into points in a space, allowing for efficient search and analysis. Embeddings can be used for various tasks such as clustering, similarity search, or recommendation systems.\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"\\nAn embedding is a method to represent high-dimensional data in a lower-dimensional space while preserving the structure of the data. This allows for efficient data processing and analysis as it reduces the complexity of the original dataset. Some popular embedding techniques include Word2Vec, GloVe, and fastText for text data, and t-SNE and UMAP for visualizing high-dimensional data in lower dimensions.\\n\\nIn the context of AI and language models, embeddings are often used to map words or sentences into a vector space where semantic meanings can be captured and compared effectively. Embeddings serve as an essential component for various NLP tasks such as question answering, sentiment analysis, text generation, and more.\\n\\nHere's a Python example using HuggingFaceInstructEmbeddings to demonstrate the use of embeddings:\\n\\n```python\\nimport os\\nfrom langchain_community import embeddings\\nfrom HuggingFaceInstructEmbeddings import HuggingFaceInstructEmbeddings\\n\\ndef main():\\n    # Load the embedding model\\n    embedding_model = HuggingFaceInstructEmbeddings.load(\\\"text-embedding-ada-multilingual-10k\\\")\\n\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"\\nAn embedding is a mathematical representation of a dataset where each element in the dataset is transformed into a fixed-size vector.\\n\\nIn the context of natural language processing (NLP), embeddings are used to represent words and sentences as dense vectors that capture their meaning, which can then be used for various downstream tasks like text classification, information retrieval, etc. There are different types of embeddings such as Word Embeddings, Sentence Embeddings, and Document Embeddings.\\n\\nSome popular embedding models include Word2Vec, GloVe, FastText, BERT, ELMO, and XLNet. These models are trained on massive datasets to learn contextualized vector representations for words and sentences. The resulting embeddings can be used in various NLP tasks like sentiment analysis, question answering systems, machine translation, etc.\\n\\nIn summary, an embedding is a way of representing text data in a low-dimensional space, where each word or document is assigned a unique fixed-size vector that reflects its meaning. These vectors are then utilized for numerous downstream tasks within the NLP domain.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 9:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"what is an embedding?\",\n",
      "  \"summaries\": \"Content:  An embedding is a technique that converts data from one representation to another. It's like taking a 3D model and projecting it onto a 2D surface, where each object is given a unique position in the new space according to its features. In the context of natural language processing (NLP), embeddings are particularly useful for capturing the semantic meaning of words or phrases. For instance, an embedding could be used to represent a sentence as a vector, with different components representing aspects like semantics, syntax, and even sentiment. This makes it easier to compare and analyze vast amounts of text data in a way that's suitable for machine learning algorithms.\\nSource: https://python.langchain.com/docs/integrations/platforms/huggingface\\n\\nContent:  An embedding is a representation of data in which the elements within the dataset are transformed into points in a space, allowing for efficient search and analysis. Embeddings can be used for various tasks such as clustering, similarity search, or recommendation systems.\\nSource: https://python.langchain.com/docs/integrations/platforms/huggingface\\n\\nContent: \\nAn embedding is a method to represent high-dimensional data in a lower-dimensional space while preserving the structure of the data. This allows for efficient data processing and analysis as it reduces the complexity of the original dataset. Some popular embedding techniques include Word2Vec, GloVe, and fastText for text data, and t-SNE and UMAP for visualizing high-dimensional data in lower dimensions.\\n\\nIn the context of AI and language models, embeddings are often used to map words or sentences into a vector space where semantic meanings can be captured and compared effectively. Embeddings serve as an essential component for various NLP tasks such as question answering, sentiment analysis, text generation, and more.\\n\\nHere's a Python example using HuggingFaceInstructEmbeddings to demonstrate the use of embeddings:\\n\\n```python\\nimport os\\nfrom langchain_community import embeddings\\nfrom HuggingFaceInstructEmbeddings import HuggingFaceInstructEmbeddings\\n\\ndef main():\\n    # Load the embedding model\\n    embedding_model = HuggingFaceInstructEmbeddings.load(\\\"text-embedding-ada-multilingual-10k\\\")\\n\\nSource: https://python.langchain.com/docs/integrations/platforms/huggingface\\n\\nContent: \\nAn embedding is a mathematical representation of a dataset where each element in the dataset is transformed into a fixed-size vector.\\n\\nIn the context of natural language processing (NLP), embeddings are used to represent words and sentences as dense vectors that capture their meaning, which can then be used for various downstream tasks like text classification, information retrieval, etc. There are different types of embeddings such as Word Embeddings, Sentence Embeddings, and Document Embeddings.\\n\\nSome popular embedding models include Word2Vec, GloVe, FastText, BERT, ELMO, and XLNet. These models are trained on massive datasets to learn contextualized vector representations for words and sentences. The resulting embeddings can be used in various NLP tasks like sentiment analysis, question answering systems, machine translation, etc.\\n\\nIn summary, an embedding is a way of representing text data in a low-dimensional space, where each word or document is assigned a unique fixed-size vector that reflects its meaning. These vectors are then utilized for numerous downstream tasks within the NLP domain.\\nSource: https://python.langchain.com/docs/integrations/platforms/huggingface\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 9:chain:LLMChain > 10:llm:LlamaCpp] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Given the following extracted parts of a long document and a question, create a final answer with references (\\\"SOURCES\\\"). \\nIf you don't know the answer, just say that you don't know. Don't try to make up an answer.\\nALWAYS return a \\\"SOURCES\\\" part in your answer.\\n\\nQUESTION: Which state/country's law governs the interpretation of the contract?\\n=========\\nContent: This Agreement is governed by English law and the parties submit to the exclusive jurisdiction of the English courts in  relation to any dispute (contractual or non-contractual) concerning this Agreement save that either party may apply to any court for an  injunction or other relief to protect its Intellectual Property Rights.\\nSource: 28-pl\\nContent: No Waiver. Failure or delay in exercising any right or remedy under this Agreement shall not constitute a waiver of such (or any other)  right or remedy.\\n\\n11.7 Severability. The invalidity, illegality or unenforceability of any term (or part of a term) of this Agreement shall not affect the continuation  in force of the remainder of the term (if any) and this Agreement.\\n\\n11.8 No Agency. Except as expressly stated otherwise, nothing in this Agreement shall create an agency, partnership or joint venture of any  kind between the parties.\\n\\n11.9 No Third-Party Beneficiaries.\\nSource: 30-pl\\nContent: (b) if Google believes, in good faith, that the Distributor has violated or caused Google to violate any Anti-Bribery Laws (as  defined in Clause 8.5) or that such a violation is reasonably likely to occur,\\nSource: 4-pl\\n=========\\nFINAL ANSWER: This Agreement is governed by English law.\\nSOURCES: 28-pl\\n\\nQUESTION: What did the president say about Michael Jackson?\\n=========\\nContent: Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\n\\nSix days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\n\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\n\\nHe met the Ukrainian people. \\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. \\n\\nGroups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland.\\nSource: 0-pl\\nContent: And we won’t stop. \\n\\nWe have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life. \\n\\nLet’s use this moment to reset. Let’s stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.  \\n\\nLet’s stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.  \\n\\nWe can’t change how divided we’ve been. But we can change how we move forward—on COVID-19 and other issues we must face together. \\n\\nI recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. \\n\\nThey were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \\n\\nOfficer Mora was 27 years old. \\n\\nOfficer Rivera was 22. \\n\\nBoth Dominican Americans who’d grown up on the same streets they later chose to patrol as police officers. \\n\\nI spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves.\\nSource: 24-pl\\nContent: And a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.  \\n\\nTo all Americans, I will be honest with you, as I’ve always promised. A Russian dictator, invading a foreign country, has costs around the world. \\n\\nAnd I’m taking robust action to make sure the pain of our sanctions  is targeted at Russia’s economy. And I will use every tool at our disposal to protect American businesses and consumers. \\n\\nTonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.  \\n\\nAmerica will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.  \\n\\nThese steps will help blunt gas prices here at home. And I know the news about what’s happening can seem alarming. \\n\\nBut I want you to know that we are going to be okay.\\nSource: 5-pl\\nContent: More support for patients and families. \\n\\nTo get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health. \\n\\nIt’s based on DARPA—the Defense Department project that led to the Internet, GPS, and so much more.  \\n\\nARPA-H will have a singular purpose—to drive breakthroughs in cancer, Alzheimer’s, diabetes, and more. \\n\\nA unity agenda for the nation. \\n\\nWe can do this. \\n\\nMy fellow Americans—tonight , we have gathered in a sacred space—the citadel of our democracy. \\n\\nIn this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done great things. \\n\\nWe have fought for freedom, expanded liberty, defeated totalitarianism and terror. \\n\\nAnd built the strongest, freest, and most prosperous nation the world has ever known. \\n\\nNow is the hour. \\n\\nOur moment of responsibility. \\n\\nOur test of resolve and conscience, of history itself. \\n\\nIt is in this moment that our character is formed. Our purpose is found. Our future is forged. \\n\\nWell I know this nation.\\nSource: 34-pl\\n=========\\nFINAL ANSWER: The president did not mention Michael Jackson.\\nSOURCES:\\n\\nQUESTION: what is an embedding?\\n=========\\nContent:  An embedding is a technique that converts data from one representation to another. It's like taking a 3D model and projecting it onto a 2D surface, where each object is given a unique position in the new space according to its features. In the context of natural language processing (NLP), embeddings are particularly useful for capturing the semantic meaning of words or phrases. For instance, an embedding could be used to represent a sentence as a vector, with different components representing aspects like semantics, syntax, and even sentiment. This makes it easier to compare and analyze vast amounts of text data in a way that's suitable for machine learning algorithms.\\nSource: https://python.langchain.com/docs/integrations/platforms/huggingface\\n\\nContent:  An embedding is a representation of data in which the elements within the dataset are transformed into points in a space, allowing for efficient search and analysis. Embeddings can be used for various tasks such as clustering, similarity search, or recommendation systems.\\nSource: https://python.langchain.com/docs/integrations/platforms/huggingface\\n\\nContent: \\nAn embedding is a method to represent high-dimensional data in a lower-dimensional space while preserving the structure of the data. This allows for efficient data processing and analysis as it reduces the complexity of the original dataset. Some popular embedding techniques include Word2Vec, GloVe, and fastText for text data, and t-SNE and UMAP for visualizing high-dimensional data in lower dimensions.\\n\\nIn the context of AI and language models, embeddings are often used to map words or sentences into a vector space where semantic meanings can be captured and compared effectively. Embeddings serve as an essential component for various NLP tasks such as question answering, sentiment analysis, text generation, and more.\\n\\nHere's a Python example using HuggingFaceInstructEmbeddings to demonstrate the use of embeddings:\\n\\n```python\\nimport os\\nfrom langchain_community import embeddings\\nfrom HuggingFaceInstructEmbeddings import HuggingFaceInstructEmbeddings\\n\\ndef main():\\n    # Load the embedding model\\n    embedding_model = HuggingFaceInstructEmbeddings.load(\\\"text-embedding-ada-multilingual-10k\\\")\\n\\nSource: https://python.langchain.com/docs/integrations/platforms/huggingface\\n\\nContent: \\nAn embedding is a mathematical representation of a dataset where each element in the dataset is transformed into a fixed-size vector.\\n\\nIn the context of natural language processing (NLP), embeddings are used to represent words and sentences as dense vectors that capture their meaning, which can then be used for various downstream tasks like text classification, information retrieval, etc. There are different types of embeddings such as Word Embeddings, Sentence Embeddings, and Document Embeddings.\\n\\nSome popular embedding models include Word2Vec, GloVe, FastText, BERT, ELMO, and XLNet. These models are trained on massive datasets to learn contextualized vector representations for words and sentences. The resulting embeddings can be used in various NLP tasks like sentiment analysis, question answering systems, machine translation, etc.\\n\\nIn summary, an embedding is a way of representing text data in a low-dimensional space, where each word or document is assigned a unique fixed-size vector that reflects its meaning. These vectors are then utilized for numerous downstream tasks within the NLP domain.\\nSource: https://python.langchain.com/docs/integrations/platforms/huggingface\\n=========\\nFINAL ANSWER:\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 9:chain:LLMChain > 10:llm:LlamaCpp] [249.83s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" An embedding is a mathematical representation of data, used in natural language processing to represent words and sentences as dense vectors that capture their meaning for various tasks like text classification and information retrieval. The term 'embedding' specifically refers to the process of converting data from one representation to another, often for the purpose of analysis or comparison.\\nSource: https://python.langchain.com/docs/integrations/platforms/huggingface\",\n",
      "        \"generation_info\": null\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain > 9:chain:LLMChain] [249.83s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \" An embedding is a mathematical representation of data, used in natural language processing to represent words and sentences as dense vectors that capture their meaning for various tasks like text classification and information retrieval. The term 'embedding' specifically refers to the process of converting data from one representation to another, often for the purpose of analysis or comparison.\\nSource: https://python.langchain.com/docs/integrations/platforms/huggingface\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain > 3:chain:MapReduceDocumentsChain] [348.21s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \" An embedding is a mathematical representation of data, used in natural language processing to represent words and sentences as dense vectors that capture their meaning for various tasks like text classification and information retrieval. The term 'embedding' specifically refers to the process of converting data from one representation to another, often for the purpose of analysis or comparison.\\nSource: https://python.langchain.com/docs/integrations/platforms/huggingface\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQAWithSourcesChain] [348.23s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"answer\": \" An embedding is a mathematical representation of data, used in natural language processing to represent words and sentences as dense vectors that capture their meaning for various tasks like text classification and information retrieval. The term 'embedding' specifically refers to the process of converting data from one representation to another, often for the purpose of analysis or comparison.\\n\",\n",
      "  \"sources\": \"https://python.langchain.com/docs/integrations/platforms/huggingface\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': \" An embedding is a mathematical representation of data, used in natural language processing to represent words and sentences as dense vectors that capture their meaning for various tasks like text classification and information retrieval. The term 'embedding' specifically refers to the process of converting data from one representation to another, often for the purpose of analysis or comparison.\\n\",\n",
       " 'sources': 'https://python.langchain.com/docs/integrations/platforms/huggingface'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"what is an embedding?\"\n",
    "# query = \"what are the main features of punch iCNG?\"\n",
    "\n",
    "langchain.debug=True\n",
    "\n",
    "chain({\"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5b58a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
